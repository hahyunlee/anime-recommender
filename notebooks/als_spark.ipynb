{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import findspark\n",
    "# Find Spark Locally\n",
    "location = findspark.find()\n",
    "findspark.init(location, edit_rc=True)\n",
    "\n",
    "import pyspark as ps    # for the pyspark suite\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, DateType, TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"anime recommender\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_filename = '../data/anime.csv'\n",
    "ratings_filename = '../data/rating.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_raw = sc.textFile(anime_filename)\n",
    "ratings_raw = sc.textFile(ratings_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_rdd = anime_raw.filter(lambda x: x is not None).filter(lambda x: x != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_raw.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = anime_raw.take(5)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ '{}'.format(x) for x in list(csv.reader([test_string], delimiter=',', quotechar='\"'))[0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.split(r',(?=\")', test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_string.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_id, name, genre, type, episodes, rating, members = [ '{}'.format(x) for x in list(csv.reader([test_string], delimiter=',', quotechar='\"'))[0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_id = int(anime_id)\n",
    "episodes = int(episodes)\n",
    "rating = float(rating)\n",
    "members = int(members)\n",
    "\n",
    "print([anime_id,episodes,rating,members])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(anime_id, name, type, episodes, rating,members, token) for token in genre.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_anime_data(input_string):\n",
    "    anime_id, name, genre, type, episodes, rating, members = [ '{}'.format(x) for x in list(csv.reader([input_string], delimiter=',', quotechar='\"'))[0] ]\n",
    "#     anime_id = int(anime_id)\n",
    "#     episodes = int(episodes)\n",
    "#     rating = float(rating)\n",
    "#     members = int(members)\n",
    "    return [(anime_id, name, type,rating,members, token) for token in genre.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_clean = anime_raw.flatMap(clean_anime_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = test_rdd.flatMap(clean_anime_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(anime_clean.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_schema = StructType( [\n",
    "    StructField('anime_id',IntegerType(),True),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('type',StringType(),True),\n",
    "    StructField('rating',FloatType(),True),\n",
    "    StructField('members',IntegerType(),True),\n",
    "    StructField('genre',StringType(),True) ] )\n",
    "\n",
    "anime = spark.createDataFrame(anime_clean, anime_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot movie genres\n",
    "anime = anime.groupBy(\"anime_id\", \"name\", \"type\",\"rating\",\"members\")\\\n",
    "               .pivot(\"genre\")\\\n",
    "               .agg(F.count(F.col('genre')))\\\n",
    "               .na.fill(0)\n",
    "\n",
    "anime.show(5)\n",
    "anime.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime_df = spark.read.load(anime_filename,format='csv',header=True,inferSchema=True)\n",
    "# ratings_df = spark.read.load(ratings_filename,format='csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlsRecommender:\n",
    "    \"\"\"\n",
    "    This a collaborative filtering recommender with Alternating Least Square\n",
    "    Matrix Factorization, which is implemented by Spark\n",
    "    \"\"\"\n",
    "    def __init__(self, spark_session, path_movies, path_ratings):\n",
    "        self.spark = spark_session\n",
    "        self.sc = spark_session.sparkContext\n",
    "        self.moviesDF = self._load_file(path_movies) \\\n",
    "            .select(['movieId', 'title'])\n",
    "        self.ratingsDF = self._load_file(path_ratings) \\\n",
    "            .select(['userId', 'movieId', 'rating'])\n",
    "        self.model = ALS(\n",
    "            userCol='userId',\n",
    "            itemCol='movieId',\n",
    "            ratingCol='rating',\n",
    "            coldStartStrategy=\"drop\")\n",
    "\n",
    "    def tune_model(self, maxIter, regParams, ranks, split_ratio=(6, 2, 2)):\n",
    "        \"\"\"\n",
    "        Hyperparameter tuning for ALS model\n",
    "        Parameters\n",
    "        ----------\n",
    "        maxIter: int, max number of learning iterations\n",
    "        regParams: list of float, regularization parameter\n",
    "        ranks: list of float, number of latent factors\n",
    "        split_ratio: tuple, (train, validation, test)\n",
    "        \"\"\"\n",
    "        # split data\n",
    "        train, val, test = self.ratingsDF.randomSplit(split_ratio)\n",
    "        # holdout tuning\n",
    "        self.model = tune_ALS(self.model, train, val,\n",
    "                              maxIter, regParams, ranks)\n",
    "        # test model\n",
    "        predictions = self.model.transform(test)\n",
    "        evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                        labelCol=\"rating\",\n",
    "                                        predictionCol=\"prediction\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        print('The out-of-sample RMSE of the best tuned model is:', rmse)\n",
    "        # clean up\n",
    "        del train, val, test, predictions, evaluator\n",
    "        gc.collect()\n",
    "\n",
    "    def set_model_params(self, maxIter, regParam, rank):\n",
    "        \"\"\"\n",
    "        set model params for pyspark.ml.recommendation.ALS\n",
    "        Parameters\n",
    "        ----------\n",
    "        maxIter: int, max number of learning iterations\n",
    "        regParams: float, regularization parameter\n",
    "        ranks: float, number of latent factors\n",
    "        \"\"\"\n",
    "        self.model = self.model \\\n",
    "            .setMaxIter(maxIter) \\\n",
    "            .setRank(rank) \\\n",
    "            .setRegParam(regParam)\n",
    "\n",
    "    def _regex_matching(self, fav_movie):\n",
    "        \"\"\"\n",
    "        return the closest matches via SQL regex.\n",
    "        If no match found, return None\n",
    "        Parameters\n",
    "        ----------\n",
    "        fav_movie: str, name of user input movie\n",
    "        Return\n",
    "        ------\n",
    "        list of indices of the matching movies\n",
    "        \"\"\"\n",
    "        print('You have input movie:', fav_movie)\n",
    "        matchesDF = self.moviesDF \\\n",
    "            .filter(\n",
    "                lower(\n",
    "                    col('title')\n",
    "                ).like('%{}%'.format(fav_movie.lower()))\n",
    "            ) \\\n",
    "            .select('movieId', 'title')\n",
    "        if not len(matchesDF.take(1)):\n",
    "            print('Oops! No match is found')\n",
    "        else:\n",
    "            movieIds = matchesDF.rdd.map(lambda r: r[0]).collect()\n",
    "            titles = matchesDF.rdd.map(lambda r: r[1]).collect()\n",
    "            print('Found possible matches in our database: '\n",
    "                  '{0}\\n'.format([x for x in titles]))\n",
    "            return movieIds\n",
    "\n",
    "    def _append_ratings(self, userId, movieIds):\n",
    "        \"\"\"\n",
    "        append a user's movie ratings to ratingsDF\n",
    "        Parameter\n",
    "        ---------\n",
    "        userId: int, userId of a user\n",
    "        movieIds: int, movieIds of user's favorite movies\n",
    "        \"\"\"\n",
    "        # create new user rdd\n",
    "        user_rdd = self.sc.parallelize(\n",
    "            [(userId, movieId, 5.0) for movieId in movieIds])\n",
    "        # transform to user rows\n",
    "        user_rows = user_rdd.map(\n",
    "            lambda x: Row(\n",
    "                userId=int(x[0]),\n",
    "                movieId=int(x[1]),\n",
    "                rating=float(x[2])\n",
    "            )\n",
    "        )\n",
    "        # transform rows to spark DF\n",
    "        userDF = self.spark.createDataFrame(user_rows) \\\n",
    "            .select(self.ratingsDF.columns)\n",
    "        # append to ratingsDF\n",
    "        self.ratingsDF = self.ratingsDF.union(userDF)\n",
    "\n",
    "    def _create_inference_data(self, userId, movieIds):\n",
    "        \"\"\"\n",
    "        create a user with all movies except ones were rated for inferencing\n",
    "        \"\"\"\n",
    "        # filter movies\n",
    "        other_movieIds = self.moviesDF \\\n",
    "            .filter(~col('movieId').isin(movieIds)) \\\n",
    "            .select(['movieId']) \\\n",
    "            .rdd.map(lambda r: r[0]) \\\n",
    "            .collect()\n",
    "        # create inference rdd\n",
    "        inferenceRDD = self.sc.parallelize(\n",
    "            [(userId, movieId) for movieId in other_movieIds]\n",
    "        ).map(\n",
    "            lambda x: Row(\n",
    "                userId=int(x[0]),\n",
    "                movieId=int(x[1]),\n",
    "            )\n",
    "        )\n",
    "        # transform to inference DF\n",
    "        inferenceDF = self.spark.createDataFrame(inferenceRDD) \\\n",
    "            .select(['userId', 'movieId'])\n",
    "        return inferenceDF\n",
    "\n",
    "    def _inference(self, model, fav_movie, n_recommendations):\n",
    "        \"\"\"\n",
    "        return top n movie recommendations based on user's input movie\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: spark ALS model\n",
    "        fav_movie: str, name of user input movie\n",
    "        n_recommendations: int, top n recommendations\n",
    "        Return\n",
    "        ------\n",
    "        list of top n similar movie recommendations\n",
    "        \"\"\"\n",
    "        # create a userId\n",
    "        userId = self.ratingsDF.agg({\"userId\": \"max\"}).collect()[0][0] + 1\n",
    "        # get movieIds of favorite movies\n",
    "        movieIds = self._regex_matching(fav_movie)\n",
    "        # append new user with his/her ratings into data\n",
    "        self._append_ratings(userId, movieIds)\n",
    "        # matrix factorization\n",
    "        model = model.fit(self.ratingsDF)\n",
    "        # get data for inferencing\n",
    "        inferenceDF = self._create_inference_data(userId, movieIds)\n",
    "        # make inference\n",
    "        return model.transform(inferenceDF) \\\n",
    "            .select(['movieId', 'prediction']) \\\n",
    "            .orderBy('prediction', ascending=False) \\\n",
    "            .rdd.map(lambda r: (r[0], r[1])) \\\n",
    "            .take(n_recommendations)\n",
    "\n",
    "    def make_recommendations(self, fav_movie, n_recommendations):\n",
    "        \"\"\"\n",
    "        make top n movie recommendations\n",
    "        Parameters\n",
    "        ----------\n",
    "        fav_movie: str, name of user input movie\n",
    "        n_recommendations: int, top n recommendations\n",
    "        \"\"\"\n",
    "        # make inference and get raw recommendations\n",
    "        print('Recommendation system start to make inference ...')\n",
    "        t0 = time.time()\n",
    "        raw_recommends = \\\n",
    "            self._inference(self.model, fav_movie, n_recommendations)\n",
    "        movieIds = [r[0] for r in raw_recommends]\n",
    "        scores = [r[1] for r in raw_recommends]\n",
    "        print('It took my system {:.2f}s to make inference \\n\\\n",
    "              '.format(time.time() - t0))\n",
    "        # get movie titles\n",
    "        movie_titles = self.moviesDF \\\n",
    "            .filter(col('movieId').isin(movieIds)) \\\n",
    "            .select('title') \\\n",
    "            .rdd.map(lambda r: r[0]) \\\n",
    "            .collect()\n",
    "        # print recommendations\n",
    "        print('Recommendations for {}:'.format(fav_movie))\n",
    "        for i in range(len(movie_titles)):\n",
    "            print('{0}: {1}, with rating '\n",
    "                  'of {2}'.format(i+1, movie_titles[i], scores[i]))\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    data object make loading raw files easier\n",
    "    \"\"\"\n",
    "    def __init__(self, spark_session, filepath):\n",
    "        \"\"\"\n",
    "        spark dataset constructor\n",
    "        \"\"\"\n",
    "        self.spark = spark_session\n",
    "        self.sc = spark_session.sparkContext\n",
    "        self.filepath = filepath\n",
    "        # build spark data object\n",
    "        self.RDD = self.load_file_as_RDD(self.filepath)\n",
    "        self.DF = self.load_file_as_DF(self.filepath)\n",
    "\n",
    "    def load_file_as_RDD(self, filepath):\n",
    "        ratings_RDD = self.sc.textFile(filepath)\n",
    "        header = ratings_RDD.take(1)[0]\n",
    "        return ratings_RDD \\\n",
    "            .filter(lambda line: line != header) \\\n",
    "            .map(lambda line: line.split(\",\")) \\\n",
    "            .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]))) # noqa\n",
    "\n",
    "    def load_file_as_DF(self, filepath):\n",
    "        ratings_RDD = self.load_file_as_rdd(filepath)\n",
    "        ratingsRDD = ratings_RDD.map(lambda tokens: Row(\n",
    "            userId=int(tokens[0]), movieId=int(tokens[1]), rating=float(tokens[2]))) # noqa\n",
    "        return self.spark.createDataFrame(ratingsRDD)\n",
    "\n",
    "\n",
    "def tune_ALS(model, train_data, validation_data, maxIter, regParams, ranks):\n",
    "    \"\"\"\n",
    "    grid search function to select the best model based on RMSE of\n",
    "    validation data\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: spark ML model, ALS\n",
    "    train_data: spark DF with columns ['userId', 'movieId', 'rating']\n",
    "    validation_data: spark DF with columns ['userId', 'movieId', 'rating']\n",
    "    maxIter: int, max number of learning iterations\n",
    "    regParams: list of float, one dimension of hyper-param tuning grid\n",
    "    ranks: list of float, one dimension of hyper-param tuning grid\n",
    "    Return\n",
    "    ------\n",
    "    The best fitted ALS model with lowest RMSE score on validation data\n",
    "    \"\"\"\n",
    "    # initial\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = 0\n",
    "    best_model = None\n",
    "    for rank in ranks:\n",
    "        for reg in regParams:\n",
    "            # get ALS model\n",
    "            als = model.setMaxIter(maxIter).setRank(rank).setRegParam(reg)\n",
    "            # train ALS model\n",
    "            model = als.fit(train_data)\n",
    "            # evaluate the model by computing the RMSE on the validation data\n",
    "            predictions = model.transform(validation_data)\n",
    "            evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                            labelCol=\"rating\",\n",
    "                                            predictionCol=\"prediction\")\n",
    "            rmse = evaluator.evaluate(predictions)\n",
    "            print('{} latent factors and regularization = {}: '\n",
    "                  'validation RMSE is {}'.format(rank, reg, rmse))\n",
    "            if rmse < min_error:\n",
    "                min_error = rmse\n",
    "                best_rank = rank\n",
    "                best_regularization = reg\n",
    "                best_model = model\n",
    "    print('\\nThe best model has {} latent factors and '\n",
    "          'regularization = {}'.format(best_rank, best_regularization))\n",
    "    return best_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
